import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import StandardScaler, RobustScaler
import joblib
import warnings
warnings.filterwarnings('ignore')


def debug_infinite_values(df, step_name):
    """ë¬´í•œê°’ ë””ë²„ê¹… í•¨ìˆ˜"""
    inf_cols = []
    for col in df.select_dtypes(include=[np.number]).columns:
        if np.isinf(df[col]).any():
            inf_count = np.isinf(df[col]).sum()
            inf_cols.append(f"{col}: {inf_count}ê°œ")
    
    if inf_cols:
        print(f"ğŸš¨ {step_name}ì—ì„œ ë¬´í•œê°’ ë°œê²¬:")
        for col_info in inf_cols:
            print(f"  - {col_info}")
    return len(inf_cols) > 0

class XGBoostStockPreprocessor:
    """XGBoost í•™ìŠµì„ ìœ„í•œ ì¢…ëª©ë³„ ì •ê·œí™” ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, forecast_horizon=1):
        self.forecast_horizon = forecast_horizon
        self.symbol_stats = {}
        self.global_scalers = {}
        
    def calculate_symbol_statistics(self, processed_stocks):
        """ì¢…ëª©ë³„ ê¸°ë³¸ í†µê³„ ê³„ì‚°"""
        print("ì¢…ëª©ë³„ í†µê³„ ê³„ì‚° ì¤‘...")
        
        for symbol, df in processed_stocks.items():
            # ì•ˆì „í•œ í†µê³„ ê³„ì‚° (0 ë°©ì§€)
            close_values = df['Close'].replace(0, np.nan).dropna()
            volume_values = df['Volume'].replace(0, np.nan).dropna() if 'Volume' in df.columns else pd.Series([1])
            
            stats = {
                'avg_close': close_values.mean() if len(close_values) > 0 else 1.0,
                'close_std': close_values.std() if len(close_values) > 0 else 1.0,
                'avg_volume': volume_values.mean() if len(volume_values) > 0 else 1.0,
                'volatility': close_values.pct_change().std() * np.sqrt(252) if len(close_values) > 1 else 0.1,
                'price_level': 'high' if close_values.mean() > 100 else 'low'
            }
            
            # NaN ë°©ì§€
            for key, value in stats.items():
                if pd.isna(value) or value == 0:
                    if 'avg' in key or 'std' in key:
                        stats[key] = 1.0
                    else:
                        stats[key] = 0.1
                        
            self.symbol_stats[symbol] = stats
        
        print(f"  ì²˜ë¦¬ëœ ì¢…ëª© ìˆ˜: {len(self.symbol_stats)}")
    
    def create_price_features(self, df, symbol):
        """ê°€ê²© ê¸°ë°˜ í”¼ì³ ìƒì„± (ë¡œê·¸ ìˆ˜ìµë¥  ì¤‘ì‹¬) - ë¬´í•œê°’ ë°©ì§€"""
        df_processed = df.copy()
        
        # 0ê°’ë“¤ì„ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ëŒ€ì²´ (ë¬´í•œê°’ ë°©ì§€)
        price_columns = ['Close', 'Open', 'High', 'Low']
        for col in price_columns:
            if col in df.columns:
                df_processed[col] = df_processed[col].replace(0, 1e-8)
        
        # 1. ë¡œê·¸ ìˆ˜ìµë¥  ê³„ì‚° (ì•ˆì „í•˜ê²Œ)
        for col in price_columns:
            if col in df.columns:
                # ë¡œê·¸ ìˆ˜ìµë¥  (ì•ˆì „í•œ ê³„ì‚°)
                price_ratio = df_processed[col] / df_processed[col].shift(1)
                price_ratio = price_ratio.replace([0, np.inf, -np.inf], np.nan)
                price_ratio = price_ratio.clip(lower=1e-10, upper=1e10)  # ê·¹ê°’ ì œí•œ
                df_processed[f'{col}_log_return'] = np.log(price_ratio)
                
                # ë‹¨ìˆœ ìˆ˜ìµë¥ 
                df_processed[f'{col}_return'] = df_processed[col].pct_change()
        
        # 2. ì´ë™í‰ê· ë“¤ì„ í˜„ì¬ê°€ ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ MA ê¸°ê°„ë“¤ í¬í•¨)
        ma_columns = [col for col in df.columns if col.startswith('MA_') and col != 'MACD']
        for col in ma_columns:
            if col in df.columns:
                # 0ê°’ ë°©ì§€
                ma_values = df_processed[col].replace(0, 1e-8)
                close_values = df_processed['Close'].replace(0, 1e-8)
                
                df_processed[f'{col}_ratio'] = ma_values / close_values
                df_processed[f'{col}_distance'] = (close_values - ma_values) / close_values
        
        # 3. EMAë“¤ë„ ë¹„ìœ¨ë¡œ ë³€í™˜ (ìƒˆë¡œìš´ EMA ê¸°ê°„ë“¤ í¬í•¨)
        ema_columns = [col for col in df.columns if col.startswith('EMA_')]
        for col in ema_columns:
            if col in df.columns:
                ema_values = df_processed[col].replace(0, 1e-8)
                close_values = df_processed['Close'].replace(0, 1e-8)
                df_processed[f'{col}_ratio'] = ema_values / close_values
        
        # 4. ë³¼ë¦°ì € ë°´ë“œë¥¼ ìƒëŒ€ì  ìœ„ì¹˜ë¡œ ë³€í™˜
        if 'BB_Upper_20' in df.columns and 'BB_Lower_20' in df.columns:
            # ë³¼ë¦°ì €ë°´ë“œ í­ ê³„ì‚° (0 ë°©ì§€)
            bb_width = df_processed['BB_Upper_20'] - df_processed['BB_Lower_20']
            bb_width = bb_width.replace(0, 1e-8)
            
            # ë³¼ë¦°ì €ë°´ë“œ ë‚´ ìƒëŒ€ì  ìœ„ì¹˜
            df_processed['BB_relative_position'] = (
                (df_processed['Close'] - df_processed['BB_Lower_20']) / bb_width
            ).fillna(0.5)
            
            # ê° ë°´ë“œ ëŒ€ë¹„ ê±°ë¦¬ (ì•ˆì „í•˜ê²Œ)
            close_values = df_processed['Close'].replace(0, 1e-8)
            df_processed['BB_upper_distance'] = (df_processed['BB_Upper_20'] - close_values) / close_values
            df_processed['BB_lower_distance'] = (close_values - df_processed['BB_Lower_20']) / close_values
        
        # 5. ê±°ë˜ëŸ‰ ì •ê·œí™” (ì•ˆì „í•˜ê²Œ)
        if 'Volume' in df.columns:
            avg_volume = self.symbol_stats[symbol]['avg_volume']
            volume_values = df_processed['Volume'].replace(0, 1e-8)
            
            df_processed['Volume_normalized'] = volume_values / avg_volume
            df_processed['Volume_log_ratio'] = np.log(volume_values / avg_volume + 1e-8)
        
        # ë¬´í•œê°’ ë° ê·¹ê°’ ì²˜ë¦¬
        numeric_columns = df_processed.select_dtypes(include=[np.number]).columns
        for col in numeric_columns:
            df_processed[col] = df_processed[col].replace([np.inf, -np.inf], np.nan)
            if df_processed[col].notna().sum() > 0:
                # ê·¹ê°’ í´ë¦¬í•‘ (99.9% / 0.1% percentile)
                p99 = df_processed[col].quantile(0.999)
                p01 = df_processed[col].quantile(0.001)
                if not pd.isna(p99) and not pd.isna(p01):
                    df_processed[col] = df_processed[col].clip(lower=p01, upper=p99)
        
        return df_processed
    
    def create_technical_features(self, df):
        """ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ ì¶”ê°€ í”¼ì³ ìƒì„±"""
        df_enhanced = df.copy()
        
        # 1. RSI ê¸°ë°˜ í”¼ì³ (ë‹¤ì–‘í•œ ê¸°ê°„ì˜ RSI ì²˜ë¦¬)
        rsi_columns = [col for col in df.columns if col.startswith('RSI_')]
        for rsi_col in rsi_columns:
            if rsi_col in df.columns:
                df_enhanced[f'{rsi_col}_overbought'] = (df[rsi_col] > 70).astype(int)
                df_enhanced[f'{rsi_col}_oversold'] = (df[rsi_col] < 30).astype(int)
                df_enhanced[f'{rsi_col}_normalized'] = (df[rsi_col] - 50) / 50  # -1 to 1 ë²”ìœ„
        
        # 2. MACD ê¸°ë°˜ í”¼ì³ (ë‹¤ì–‘í•œ MACD ë³€í˜•ë“¤ ì²˜ë¦¬)
        macd_columns = [col for col in df.columns if col.startswith('MACD') and not col.endswith('_Signal')]
        for macd_col in macd_columns:
            signal_col = f'{macd_col}_Signal'
            if macd_col in df.columns and signal_col in df.columns:
                df_enhanced[f'{macd_col}_above_signal'] = (df[macd_col] > df[signal_col]).astype(int)
                df_enhanced[f'{macd_col}_signal_distance'] = df[macd_col] - df[signal_col]
        
        # 3. ìŠ¤í† ìºìŠ¤í‹± ê¸°ë°˜ í”¼ì³ (ë‹¤ì–‘í•œ ê¸°ê°„ ì²˜ë¦¬)
        stoch_k_columns = [col for col in df.columns if col.startswith('Stoch_K_')]
        for k_col in stoch_k_columns:
            period = k_col.replace('Stoch_K_', '')
            d_col = f'Stoch_D_{period}'
            if k_col in df.columns and d_col in df.columns:
                df_enhanced[f'Stoch_{period}_K_above_D'] = (df[k_col] > df[d_col]).astype(int)
                df_enhanced[f'Stoch_{period}_overbought'] = ((df[k_col] > 80) & (df[d_col] > 80)).astype(int)
                df_enhanced[f'Stoch_{period}_oversold'] = ((df[k_col] < 20) & (df[d_col] < 20)).astype(int)
        
        # 4. Williams %R ê¸°ë°˜ í”¼ì³ (ë‹¤ì–‘í•œ ê¸°ê°„ ì²˜ë¦¬)
        williams_columns = [col for col in df.columns if col.startswith('Williams_R_')]
        for wr_col in williams_columns:
            if wr_col in df.columns:
                df_enhanced[f'{wr_col}_overbought'] = (df[wr_col] > -20).astype(int)
                df_enhanced[f'{wr_col}_oversold'] = (df[wr_col] < -80).astype(int)
        
        # 5. CCI ê¸°ë°˜ í”¼ì³ (ë‹¤ì–‘í•œ ê¸°ê°„ ì²˜ë¦¬)
        cci_columns = [col for col in df.columns if col.startswith('CCI_')]
        for cci_col in cci_columns:
            if cci_col in df.columns:
                df_enhanced[f'{cci_col}_overbought'] = (df[cci_col] > 100).astype(int)
                df_enhanced[f'{cci_col}_oversold'] = (df[cci_col] < -100).astype(int)
        
        # 6. ë³€ë™ì„± í”¼ì³ (ì•ˆì „í•˜ê²Œ)
        if 'Close_log_return' in df_enhanced.columns:
            # ì‹¤í˜„ ë³€ë™ì„± (rolling) - NaN ì²˜ë¦¬
            log_returns = df_enhanced['Close_log_return'].fillna(0)
            df_enhanced['realized_volatility_5'] = log_returns.rolling(5, min_periods=1).std()
            df_enhanced['realized_volatility_20'] = log_returns.rolling(20, min_periods=1).std()
            
            # ë³€ë™ì„± ë ˆì§ (ê³ ë³€ë™ì„±/ì €ë³€ë™ì„±)
            vol_20 = df_enhanced['realized_volatility_20'].fillna(0)
            vol_median = vol_20.median() if vol_20.sum() > 0 else 0
            df_enhanced['high_volatility_regime'] = (vol_20 > vol_median).astype(int)
        
        # 7. ì¶”ì„¸ ê°•ë„ í”¼ì³
        if 'MA_20_ratio' in df_enhanced.columns and 'MA_50_ratio' in df_enhanced.columns:
            df_enhanced['trend_strength'] = df_enhanced['MA_20_ratio'] - df_enhanced['MA_50_ratio']
            df_enhanced['strong_uptrend'] = (df_enhanced['trend_strength'] > 0.02).astype(int)
            df_enhanced['strong_downtrend'] = (df_enhanced['trend_strength'] < -0.02).astype(int)
        
        return df_enhanced
    
    def create_time_features(self, df):
        """ì‹œê°„ ê¸°ë°˜ í”¼ì³ ìƒì„±"""
        df_time = df.copy()
        
        # Date ì»¬ëŸ¼ í™•ì‹¤íˆ ìƒì„±
        if isinstance(df.index, pd.DatetimeIndex):
            df_time['Date'] = df.index
            dates = df.index
        elif 'Date' in df.columns:
            dates = pd.to_datetime(df['Date'])
        else:
            return df_time
        
        df_time['year'] = dates.year
        df_time['month'] = dates.month
        df_time['quarter'] = dates.quarter
        df_time['day_of_week'] = dates.dayofweek
        df_time['is_month_start'] = dates.is_month_start.astype(int)
        df_time['is_month_end'] = dates.is_month_end.astype(int)
        df_time['is_quarter_start'] = dates.is_quarter_start.astype(int)
        df_time['is_quarter_end'] = dates.is_quarter_end.astype(int)
        
        # ìˆœí™˜ ì¸ì½”ë”© (ì›”, ìš”ì¼)
        df_time['month_sin'] = np.sin(2 * np.pi * df_time['month'] / 12)
        df_time['month_cos'] = np.cos(2 * np.pi * df_time['month'] / 12)
        df_time['dow_sin'] = np.sin(2 * np.pi * df_time['day_of_week'] / 7)
        df_time['dow_cos'] = np.cos(2 * np.pi * df_time['day_of_week'] / 7)
        
        return df_time
    
    def prepare_single_symbol(self, df, symbol):
        """ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ì „ì²˜ë¦¬"""
        # 1. ê°€ê²© ê¸°ë°˜ í”¼ì³ ìƒì„±
        df_price = self.create_price_features(df, symbol)
        
        # 2. ê¸°ìˆ ì  ì§€í‘œ í”¼ì³ ìƒì„±
        df_technical = self.create_technical_features(df_price)
        
        # 3. ì‹œê°„ í”¼ì³ ìƒì„±
        df_complete = self.create_time_features(df_technical)
        
        # 4. íƒ€ê²Ÿ ë¼ë²¨ ìƒì„± (Future_Label ìƒì„±)
        if 'Optimized_Label' in df_complete.columns:
            df_complete['Future_Label'] = df_complete['Optimized_Label'].shift(-self.forecast_horizon)
        else:
            print("no Optimized_label... : 2ì°¨ë¼ë²¨ ìƒì„±ì´ ì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # 5. ì¢…ëª© ì •ë³´ ì¶”ê°€
        df_complete['Symbol'] = symbol
        
        return df_complete
    
    def apply_global_scaling(self, combined_df):
        """ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ê¸€ë¡œë²Œ ìŠ¤ì¼€ì¼ë§"""
        df_scaled = combined_df.copy()
        
        # ìŠ¤ì¼€ì¼ë§ ì „ ë¬´í•œê°’ ë° ì´ìƒì¹˜ ì²˜ë¦¬
        print("ë¬´í•œê°’ ë° ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        # 1. ë¬´í•œê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
        df_scaled = df_scaled.replace([np.inf, -np.inf], np.nan)
        
        # 2. ë§¤ìš° í° ê°’ë“¤ í´ë¦¬í•‘ (99.9% percentile ê¸°ì¤€)
        numeric_cols = df_scaled.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if df_scaled[col].notna().sum() > 0:  # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                p99 = df_scaled[col].quantile(0.999)
                p01 = df_scaled[col].quantile(0.001)
                if not pd.isna(p99) and not pd.isna(p01) and p99 != p01:
                    df_scaled[col] = df_scaled[col].clip(lower=p01, upper=p99)
        
        # 3. ë‚¨ì€ NaN ì²˜ë¦¬ (ìŠ¤ì¼€ì¼ë§ ì „ì—)
        df_scaled = df_scaled.fillna(0)
        
        print(f"  ì²˜ë¦¬ í›„ ë¬´í•œê°’ ìˆ˜: {np.isinf(df_scaled.select_dtypes(include=[np.number])).sum().sum()}")
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ë“¤ ë¨¼ì € ì‹ë³„
        string_columns = df_scaled.select_dtypes(include=['object']).columns.tolist()
        
        # ìŠ¤ì¼€ì¼ë§í•  í”¼ì³ ê·¸ë£¹ ë¶„ë¥˜
        log_return_features = [col for col in df_scaled.columns if '_log_return' in col]
        return_features = [col for col in df_scaled.columns if '_return' in col and '_log_return' not in col]
        ratio_features = [col for col in df_scaled.columns if '_ratio' in col]
        distance_features = [col for col in df_scaled.columns if '_distance' in col]
        volatility_features = [col for col in df_scaled.columns if 'volatility' in col or 'Volatility' in col]
        
        # SIGNAL_FEATURESì— ì •ì˜ëœ ì •ê·œí™” ì œì™¸ íŠ¹ì„±ë“¤
        normalized_features = SIGNAL_FEATURES.copy()
        
        # ë‚˜ë¨¸ì§€ ê¸°ìˆ ì  ì§€í‘œë“¤
        exclude_cols = (EXCLUDE_FROM_TRAINING + log_return_features + return_features + 
                    ratio_features + distance_features + volatility_features + normalized_features +
                    ['year', 'month', 'quarter', 'day_of_week'] + string_columns)
        
        other_features = [col for col in df_scaled.columns if col not in exclude_cols]
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        if not self.global_scalers:
            self.global_scalers = {
                'log_return_scaler': StandardScaler(),
                'return_scaler': RobustScaler(quantile_range=(1, 99)),
                'ratio_scaler': StandardScaler(),
                'distance_scaler': StandardScaler(),
                'volatility_scaler': RobustScaler(quantile_range=(5, 95)),
                'other_scaler': StandardScaler()
            }
        
        # ê° ê·¸ë£¹ë³„ ìŠ¤ì¼€ì¼ë§ (ì•ˆì „í•˜ê²Œ)
        def safe_scaling(scaler, data, feature_names, scaler_name):
            if feature_names and len(feature_names) > 0:
                clean_data = data[feature_names].fillna(0)
                # ì¶”ê°€ ë¬´í•œê°’ ì²´í¬
                if np.isinf(clean_data).any().any():
                    print(f"  âš ï¸ {scaler_name}ì—ì„œ ë¬´í•œê°’ ë°œê²¬, 0ìœ¼ë¡œ ëŒ€ì²´")
                    clean_data = clean_data.replace([np.inf, -np.inf], 0)
                
                try:
                    scaled_values = scaler.fit_transform(clean_data)
                    data[feature_names] = scaled_values
                    print(f"  âœ… {scaler_name}: {len(feature_names)}ê°œ í”¼ì³ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
                except Exception as e:
                    print(f"  âŒ {scaler_name} ìŠ¤ì¼€ì¼ë§ ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨ì‹œ 0ìœ¼ë¡œ ëŒ€ì²´
                    data[feature_names] = 0
            return data
        
        df_scaled = safe_scaling(self.global_scalers['log_return_scaler'], df_scaled, 
                               log_return_features, "ë¡œê·¸ìˆ˜ìµë¥ ")
        df_scaled = safe_scaling(self.global_scalers['return_scaler'], df_scaled, 
                               return_features, "ì¼ë°˜ìˆ˜ìµë¥ ")
        df_scaled = safe_scaling(self.global_scalers['ratio_scaler'], df_scaled, 
                               ratio_features, "ë¹„ìœ¨")
        df_scaled = safe_scaling(self.global_scalers['distance_scaler'], df_scaled, 
                               distance_features, "ê±°ë¦¬")
        df_scaled = safe_scaling(self.global_scalers['volatility_scaler'], df_scaled, 
                               volatility_features, "ë³€ë™ì„±")
        df_scaled = safe_scaling(self.global_scalers['other_scaler'], df_scaled, 
                               other_features, "ê¸°íƒ€ì§€í‘œ")
        
        return df_scaled
    
    def get_training_features(self, df):
        """í•™ìŠµì— ì‚¬ìš©í•  í”¼ì³ë§Œ ì„ íƒ"""
        all_columns = df.columns.tolist()
        training_features = [col for col in all_columns if col not in EXCLUDE_FROM_TRAINING]
        return training_features
    
    def process_all_stocks(self, processed_stocks, output_dir):
        """ì „ì²´ ì¢…ëª© ì²˜ë¦¬ ë° ë°ì´í„° ë¶„í• """
        print("XGBoostìš© ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
        print("=" * 60)
        
        # 1. ì¢…ëª©ë³„ í†µê³„ ê³„ì‚°
        self.calculate_symbol_statistics(processed_stocks)
        
        # 2. ì¢…ëª©ë³„ ì „ì²˜ë¦¬
        all_data = []
        backtest_data = {}
        
        print("\nì¢…ëª©ë³„ ì „ì²˜ë¦¬ ì¤‘...")
        for symbol, df in processed_stocks.items():
            print(f"  ì²˜ë¦¬ ì¤‘: {symbol}")
            
            # ë°±í…ŒìŠ¤íŒ…ìš© ì›ë³¸ ë°ì´í„° ë³´ì¡´ (Date ì²˜ë¦¬)
            df_reset = df.reset_index()
            if 'Date' not in df_reset.columns and 'index' in df_reset.columns:
                df_reset.rename(columns={'index': 'Date'}, inplace=True)
            
            # BACKTEST_PRESERVE_COLUMNSì— ìˆëŠ” ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ
            available_cols = [col for col in BACKTEST_PRESERVE_COLUMNS if col in df_reset.columns]
            backtest_data[symbol] = df_reset[available_cols].copy()
            
            # í•™ìŠµìš© ë°ì´í„° ì „ì²˜ë¦¬
            processed_df = self.prepare_single_symbol(df, symbol)
            all_data.append(processed_df)
        
        # 3. ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
        combined_df = pd.concat(all_data, axis=0, ignore_index=True)
        print(f"\ní•©ì¹œ ë°ì´í„° í¬ê¸°: {combined_df.shape}")
        
        # 4. ê¸€ë¡œë²Œ ìŠ¤ì¼€ì¼ë§
        print("ê¸€ë¡œë²Œ ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘...")
        scaled_df = self.apply_global_scaling(combined_df)
        
        # 5. ìµœì¢… ê²°ì¸¡ì¹˜ ë° ë¬´í•œê°’ ì²˜ë¦¬
        scaled_df = scaled_df.replace([np.inf, -np.inf], np.nan)
        scaled_df = scaled_df.dropna(subset=['Future_Label'])
        
        # ìµœì¢… ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        numeric_columns = scaled_df.select_dtypes(include=[np.number]).columns
        scaled_df[numeric_columns] = scaled_df[numeric_columns].fillna(0)
        
        print(f"ìµœì¢… ë°ì´í„° í¬ê¸°: {scaled_df.shape}")
        
        # 6. ë¼ë²¨ ë¶„í¬ í™•ì¸
        print(f"\në¼ë²¨ ë¶„í¬:")
        label_counts = scaled_df['Future_Label'].value_counts().sort_index()
        for label, count in label_counts.items():
            label_name = {0: 'Sell', 1: 'Hold', 2: 'Buy'}[int(label)]
            print(f"  {int(label)} ({label_name}): {count:,}ê°œ ({count/len(scaled_df)*100:.1f}%)")
        
        # 7. í•™ìŠµìš© í”¼ì³ ë¦¬ìŠ¤íŠ¸
        training_features = self.get_training_features(scaled_df)
        print(f"\ní•™ìŠµìš© í”¼ì³ ìˆ˜: {len(training_features)}")
        
        # 8. ë°ì´í„° ì €ì¥
        os.makedirs(output_dir, exist_ok=True)
        
        # ì „ì²´ ë°ì´í„° (í•™ìŠµìš©)
        scaled_df.to_csv(os.path.join(output_dir, 'xgboost_training_data.csv'), index=False)
        
        # ë°±í…ŒìŠ¤íŒ…ìš© ë°ì´í„° (ì¢…ëª©ë³„ ê°œë³„ íŒŒì¼)
        backtest_dir = os.path.join(output_dir, 'backtest_data')
        os.makedirs(backtest_dir, exist_ok=True)
        
        for symbol, data in backtest_data.items():
            data.to_csv(os.path.join(backtest_dir, f'{symbol}_backtest.csv'), index=False)
        
        # ì „ì²˜ë¦¬ê¸° ì €ì¥
        joblib.dump(self, os.path.join(output_dir, 'xgboost_preprocessor.pkl'))
        
        # í”¼ì³ ë¦¬ìŠ¤íŠ¸ ì €ì¥
        with open(os.path.join(output_dir, 'training_features.txt'), 'w') as f:
            for feature in training_features:
                f.write(f"{feature}\n")
        
        print(f"\në°ì´í„° ì €ì¥ ì™„ë£Œ:")
        print(f"  - í•™ìŠµìš©: {output_dir}/xgboost_training_data.csv")
        print(f"  - ë°±í…ŒìŠ¤íŒ…ìš©: {backtest_dir}/")
        print(f"  - ì „ì²˜ë¦¬ê¸°: {output_dir}/xgboost_preprocessor.pkl")
        print(f"  - í”¼ì³ ë¦¬ìŠ¤íŠ¸: {output_dir}/training_features.txt")
        
        return scaled_df, backtest_data, training_features

def split_time_series_data(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """ì‹œê³„ì—´ ë°ì´í„° ì‹œê°„ìˆœ ë¶„í• """
    
    # Date ê¸°ì¤€ ì •ë ¬
    if 'Date' in df.columns:
        df_sorted = df.sort_values('Date').copy()
    else:
        df_sorted = df.copy()
    
    n = len(df_sorted)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))
    
    train_df = df_sorted.iloc[:train_end].copy()
    val_df = df_sorted.iloc[train_end:val_end].copy()
    test_df = df_sorted.iloc[val_end:].copy()
    
    print(f"\në°ì´í„° ë¶„í•  ê²°ê³¼:")
    print(f"  í›ˆë ¨: {len(train_df):,}ê°œ ({len(train_df)/n*100:.1f}%)")
    print(f"  ê²€ì¦: {len(val_df):,}ê°œ ({len(val_df)/n*100:.1f}%)")
    print(f"  í…ŒìŠ¤íŠ¸: {len(test_df):,}ê°œ ({len(test_df)/n*100:.1f}%)")
    
    if 'Date' in df.columns:
        print(f"  ê¸°ê°„: {train_df['Date'].min().date()} ~ {test_df['Date'].max().date()}")
    
    return train_df, val_df, test_df

def filter_target_symbols(processed_stocks, target_symbols=None):
    """
    íƒ€ê²Ÿ ì¢…ëª©ë“¤ë§Œ í•„í„°ë§
    
    Parameters:
    -----------
    processed_stocks : dict
        ì „ì²´ ì¢…ëª©ë³„ ë°ì´í„°í”„ë ˆì„
    target_symbols : list or None
        ì„ íƒí•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ì „ì²´ ì‚¬ìš©
        ì˜ˆ: ['BTC', 'ETH', 'SOL'] (-USD ì œì™¸)
    
    Returns:
    --------
    dict : í•„í„°ë§ëœ ì¢…ëª©ë³„ ë°ì´í„°í”„ë ˆì„
    """
    if target_symbols is None:
        print("ğŸ”„ ì „ì²´ ì¢…ëª© ì‚¬ìš©")
        return processed_stocks
    
    # -USD ì œê±°í•˜ì—¬ ì •ê·œí™”
    normalized_targets = []
    for symbol in target_symbols:
        clean_symbol = symbol.replace('-USD', '').replace('_USDT', '').replace('USDT', '')
        normalized_targets.append(clean_symbol)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì¢…ëª©ë“¤ í™•ì¸
    available_symbols = list(processed_stocks.keys())
    filtered_stocks = {}
    found_symbols = []
    missing_symbols = []
    
    for target in normalized_targets:
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì‹¬ë³¼ ì°¾ê¸°
        matched = False
        for available in available_symbols:
            # ë‹¤ì–‘í•œ íŒ¨í„´ ë§¤ì¹­
            clean_available = available.replace('-USD', '').replace('_USDT', '').replace('USDT', '')
            if clean_available == target or available == target:
                filtered_stocks[available] = processed_stocks[available]
                found_symbols.append(f"{target} â†’ {available}")
                matched = True
                break
        
        if not matched:
            missing_symbols.append(target)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ğŸ¯ íƒ€ê²Ÿ ì¢…ëª© í•„í„°ë§ ê²°ê³¼:")
    print(f"  ìš”ì²­ëœ ì¢…ëª©: {len(target_symbols)}ê°œ")
    print(f"  ë°œê²¬ëœ ì¢…ëª©: {len(found_symbols)}ê°œ")
    
    if found_symbols:
        print("  âœ… ë§¤ì¹­ëœ ì¢…ëª©ë“¤:")
        for match in found_symbols:
            print(f"    - {match}")
    
    if missing_symbols:
        print(f"  âŒ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì¢…ëª©ë“¤: {missing_symbols}")
        print(f"  ğŸ’¡ ì‚¬ìš© ê°€ëŠ¥í•œ ì¢…ëª©ë“¤: {list(processed_stocks.keys())[:10]}...")
    
    return filtered_stocks

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def prepare_xgboost_data(processed_stocks, output_dir='./data/xgboost', forecast_horizon=1, target_symbols=None):
    """
    XGBoost í•™ìŠµì„ ìœ„í•œ ì™„ì „í•œ ë°ì´í„° ì „ì²˜ë¦¬
    
    Parameters:
    -----------
    processed_stocks : dict
        ì¢…ëª©ë³„ ë°ì´í„°í”„ë ˆì„ (Final_Composite_Signal í¬í•¨)
    output_dir : str
        ì¶œë ¥ ë””ë ‰í† ë¦¬
    forecast_horizon : int
        ì˜ˆì¸¡ ê¸°ê°„ (ì¼)
    target_symbols : list or None
        ì„ íƒí•  ì¢…ëª© ë¦¬ìŠ¤íŠ¸. Noneì´ë©´ ì „ì²´ ì‚¬ìš©
        ì˜ˆ: ['BTC', 'ETH', 'SOL', 'DOGE'] (-USDëŠ” ìë™ ì œê±°ë¨)
    
    Returns:
    --------
    dict : (í•™ìŠµìš©_ë°ì´í„°, ë°±í…ŒìŠ¤íŒ…ìš©_ë°ì´í„°, í”¼ì³_ë¦¬ìŠ¤íŠ¸)
    """
    
    # 1. íƒ€ê²Ÿ ì¢…ëª© í•„í„°ë§
    filtered_stocks = filter_target_symbols(processed_stocks, target_symbols)
    
    if not filtered_stocks:
        raise ValueError("âŒ ì„ íƒëœ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # 2. ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = XGBoostStockPreprocessor(forecast_horizon=forecast_horizon)
    
    # 3. ì „ì²´ ë°ì´í„° ì²˜ë¦¬
    scaled_df, backtest_data, training_features = preprocessor.process_all_stocks(
        filtered_stocks, output_dir
    )
    
    # 4. ì‹œê³„ì—´ ë¶„í• 
    train_df, val_df, test_df = split_time_series_data(scaled_df)
    
    # 5. ë¶„í• ëœ ë°ì´í„° ì €ì¥
    train_df.to_csv(os.path.join(output_dir, 'train_data.csv'), index=False)
    val_df.to_csv(os.path.join(output_dir, 'val_data.csv'), index=False)
    test_df.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False)
    
    return {
        'train': train_df,
        'val': val_df, 
        'test': test_df,
        'backtest_data': backtest_data,
        'training_features': training_features,
        'preprocessor': preprocessor
    }