{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673a272f",
   "metadata": {},
   "source": [
    "# GPU check pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c938d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PyTorch GPU 확인 ===\n",
      "PyTorch 버전: 2.0.1\n",
      "CUDA 사용 가능: True\n",
      "CUDA 버전: 11.7\n",
      "GPU 개수: 1\n",
      "현재 GPU: 0\n",
      "GPU 이름: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU 총 메모리: 11.0 GB\n",
      "할당된 메모리: 0.0 MB\n",
      "캐시된 메모리: 0.0 MB\n",
      "\n",
      "=== GPU 텐서 테스트 ===\n",
      "GPU 텐서 계산 성공: cuda:0\n",
      "\n",
      "=== nvidia-smi 출력 ===\n",
      "Sun Sep 14 17:35:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:01:00.0  On |                  N/A |\n",
      "| 36%   40C    P8             25W /  257W |    1294MiB /  11264MiB |     14%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        46      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A     70993      C   /python3.10                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=== PyTorch GPU 확인 ===\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "    print(f\"GPU 개수: {torch.cuda.device_count()}\")\n",
    "    print(f\"현재 GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # GPU 메모리 정보\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    print(f\"GPU 총 메모리: {gpu_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "    # 현재 메모리 사용량\n",
    "    allocated = torch.cuda.memory_allocated(0)\n",
    "    cached = torch.cuda.memory_reserved(0)\n",
    "    print(f\"할당된 메모리: {allocated / 1024**2:.1f} MB\")\n",
    "    print(f\"캐시된 메모리: {cached / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # 간단한 GPU 텐서 테스트\n",
    "    print(\"\\n=== GPU 텐서 테스트 ===\")\n",
    "    x = torch.randn(3, 3).cuda()\n",
    "    y = torch.randn(3, 3).cuda()\n",
    "    z = torch.matmul(x, y)\n",
    "    print(f\"GPU 텐서 계산 성공: {z.device}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ CUDA를 사용할 수 없습니다.\")\n",
    "\n",
    "print(\"\\n=== nvidia-smi 출력 ===\")\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"nvidia-smi 실행 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbec281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU 환경 확인 ===\n",
      "PyTorch CUDA: True\n",
      "XGBoost 버전: 3.0.4\n",
      "XGBoost GPU: 지원됨 (gpu_hist 사용 가능)\n",
      "LightGBM 버전: 4.6.0\n",
      "CatBoost 버전: None\n",
      "=== 대용량 데이터 GPU 테스트 ===\n",
      "대용량 데이터 생성 중... (500,000 × 100)\n",
      "훈련 데이터: (400000, 100)\n",
      "메모리 크기: 305.2 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import torch\n",
    "\n",
    "# GPU 사용 가능 확인\n",
    "print(\"=== GPU 환경 확인 ===\")\n",
    "print(f\"PyTorch CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"XGBoost 버전: {xgb.__version__}\")\n",
    "\n",
    "# XGBoost GPU 지원 확인 (다른 방법)\n",
    "try:\n",
    "    # GPU 히스토그램 방법이 지원되는지 테스트\n",
    "    temp_model = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0)\n",
    "    print(\"XGBoost GPU: 지원됨 (gpu_hist 사용 가능)\")\n",
    "except Exception as e:\n",
    "    print(f\"XGBoost GPU: 확인 불가 ({str(e)[:50]}...)\")\n",
    "\n",
    "print(f\"LightGBM 버전: {lgb.__version__}\")\n",
    "print(f\"CatBoost 버전: {CatBoostRegressor().get_param('iterations')}\")  # 버전 확인 대신\n",
    "\n",
    "# 가상 회귀 데이터 생성 (주가 데이터 시뮬레이션)\n",
    "# 대용량 데이터로 GPU 성능 확인\n",
    "print(\"=== 대용량 데이터 GPU 테스트 ===\")\n",
    "\n",
    "# 더 큰 데이터셋 생성\n",
    "n_samples_large = 500000  # 50만 개 (10배 증가)\n",
    "n_features_large = 100    # 100개 피처\n",
    "\n",
    "print(f\"대용량 데이터 생성 중... ({n_samples_large:,} × {n_features_large})\")\n",
    "np.random.seed(42)\n",
    "\n",
    "X_large = np.random.randn(n_samples_large, n_features_large)\n",
    "y_large = (100 + \n",
    "           np.sum(X_large[:, :20] * np.random.randn(20), axis=1) + \n",
    "           np.random.randn(n_samples_large) * 0.1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"훈련 데이터: {X_train.shape}\")\n",
    "print(f\"메모리 크기: {X_train.nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08837041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost 테스트 ===\n",
      "CPU 버전 학습 중...\n",
      "XGBoost CPU - 시간: 16.00초, RMSE: 0.4871, R²: 0.9887\n",
      "GPU 버전 학습 중...\n",
      "XGBoost GPU - 시간: 4.57초, RMSE: 0.4869, R²: 0.9887\n",
      "🚀 속도 향상: 3.50배\n"
     ]
    }
   ],
   "source": [
    "# 1. XGBoost CPU vs GPU 테스트\n",
    "print(\"\\n=== XGBoost 테스트 ===\")\n",
    "\n",
    "# CPU 버전\n",
    "print(\"CPU 버전 학습 중...\")\n",
    "start_time = time.time()\n",
    "xgb_cpu = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    tree_method='hist',  # CPU\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_cpu.fit(X_train, y_train)\n",
    "cpu_time = time.time() - start_time\n",
    "cpu_pred = xgb_cpu.predict(X_test)\n",
    "cpu_rmse = np.sqrt(mean_squared_error(y_test, cpu_pred))\n",
    "cpu_r2 = r2_score(y_test, cpu_pred)\n",
    "\n",
    "print(f\"XGBoost CPU - 시간: {cpu_time:.2f}초, RMSE: {cpu_rmse:.4f}, R²: {cpu_r2:.4f}\")\n",
    "\n",
    "# GPU 버전 시도\n",
    "print(\"GPU 버전 학습 중...\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    xgb_gpu = xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        tree_method='gpu_hist',  # GPU\n",
    "        gpu_id=0,\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    xgb_gpu.fit(X_train, y_train)\n",
    "    gpu_time = time.time() - start_time\n",
    "    gpu_pred = xgb_gpu.predict(X_test)\n",
    "    gpu_rmse = np.sqrt(mean_squared_error(y_test, gpu_pred))\n",
    "    gpu_r2 = r2_score(y_test, gpu_pred)\n",
    "    \n",
    "    print(f\"XGBoost GPU - 시간: {gpu_time:.2f}초, RMSE: {gpu_rmse:.4f}, R²: {gpu_r2:.4f}\")\n",
    "    print(f\"🚀 속도 향상: {cpu_time/gpu_time:.2f}배\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ XGBoost GPU 실행 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc0a94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CatBoost 테스트 ===\n",
      "CPU 버전 학습 중...\n",
      "CatBoost CPU - 시간: 18.32초, RMSE: 0.2641, R²: 0.9967\n",
      "GPU 버전 학습 중...\n",
      "CatBoost GPU - 시간: 11.33초, RMSE: 0.2773, R²: 0.9963\n",
      "🚀 속도 향상: 1.62배\n",
      "\n",
      "=== 모든 테스트 완료! ===\n"
     ]
    }
   ],
   "source": [
    "# 3. CatBoost 테스트\n",
    "print(\"\\n=== CatBoost 테스트 ===\")\n",
    "\n",
    "# CPU 버전\n",
    "print(\"CPU 버전 학습 중...\")\n",
    "start_time = time.time()\n",
    "cat_cpu = CatBoostRegressor(\n",
    "    iterations=300,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    task_type='CPU',\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_cpu.fit(X_train, y_train)\n",
    "cpu_time = time.time() - start_time\n",
    "cpu_pred = cat_cpu.predict(X_test)\n",
    "cpu_rmse = np.sqrt(mean_squared_error(y_test, cpu_pred))\n",
    "cpu_r2 = r2_score(y_test, cpu_pred)\n",
    "\n",
    "print(f\"CatBoost CPU - 시간: {cpu_time:.2f}초, RMSE: {cpu_rmse:.4f}, R²: {cpu_r2:.4f}\")\n",
    "\n",
    "# GPU 버전 시도\n",
    "print(\"GPU 버전 학습 중...\")\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    cat_gpu = CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        depth=6,\n",
    "        learning_rate=0.1,\n",
    "        task_type='GPU',\n",
    "        devices='0',\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    cat_gpu.fit(X_train, y_train)\n",
    "    gpu_time = time.time() - start_time\n",
    "    gpu_pred = cat_gpu.predict(X_test)\n",
    "    gpu_rmse = np.sqrt(mean_squared_error(y_test, gpu_pred))\n",
    "    gpu_r2 = r2_score(y_test, gpu_pred)\n",
    "    \n",
    "    print(f\"CatBoost GPU - 시간: {gpu_time:.2f}초, RMSE: {gpu_rmse:.4f}, R²: {gpu_r2:.4f}\")\n",
    "    print(f\"🚀 속도 향상: {cpu_time/gpu_time:.2f}배\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ CatBoost GPU 실행 실패: {e}\")\n",
    "\n",
    "print(\"\\n=== 모든 테스트 완료! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6198d9",
   "metadata": {},
   "source": [
    "# LGBM\n",
    "### LightGBM은 보통 CPU 전용 빌드\n",
    "- 그래서 GPU를 쓰려면 직접 소스에서 컴파일해야 합니다.\n",
    "- LightGBM GPU 버전도 있긴 하지만, 안정성과 속도 측면에서 XGBoost, CatBoost GPU 버전이 더 자주 쓰입니다.\n",
    "- 특히 CatBoost는 GPU 지원이 훨씬 잘 되어 있어서 대규모 데이터엔 선호도가 높음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07afb9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LightGBM GPU 테스트 (개선) ===\n",
      "❌ OpenCL 플랫폼 없음\n",
      "GPU 버전 학습 재시도...\n",
      "❌ 방법 1 실패: No OpenCL device found\n",
      "❌ 방법 2도 실패: CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n",
      "💡 LightGBM은 CPU 버전으로 사용하고 XGBoost와 CatBoost GPU를 활용하세요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDA=1\n"
     ]
    }
   ],
   "source": [
    "# LightGBM GPU 테스트 (개선된 버전)\n",
    "print(\"\\n=== LightGBM GPU 테스트 (개선) ===\")\n",
    "\n",
    "# OpenCL 정보 확인\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['clinfo'], capture_output=True, text=True, timeout=10)\n",
    "    if \"Platform\" in result.stdout:\n",
    "        print(\"✅ OpenCL 플랫폼 발견\")\n",
    "        print(result.stdout[:200] + \"...\")\n",
    "    else:\n",
    "        print(\"❌ OpenCL 플랫폼 없음\")\n",
    "except:\n",
    "    print(\"❌ clinfo 명령어 실행 실패\")\n",
    "\n",
    "# GPU 버전 재시도 (더 많은 옵션)\n",
    "print(\"GPU 버전 학습 재시도...\")\n",
    "try:\n",
    "    # 방법 1: 기본 GPU 설정\n",
    "    start_time = time.time()\n",
    "    lgb_gpu = lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        device='gpu',\n",
    "        gpu_use_dp=False,  # 단정밀도 사용\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_gpu.fit(X_train, y_train)\n",
    "    gpu_time = time.time() - start_time\n",
    "    gpu_pred = lgb_gpu.predict(X_test)\n",
    "    gpu_rmse = np.sqrt(mean_squared_error(y_test, gpu_pred))\n",
    "    gpu_r2 = r2_score(y_test, gpu_pred)\n",
    "    \n",
    "    print(f\"✅ LightGBM GPU - 시간: {gpu_time:.2f}초, RMSE: {gpu_rmse:.4f}, R²: {gpu_r2:.4f}\")\n",
    "    print(f\"🚀 속도 향상: {cpu_time/gpu_time:.2f}배\")\n",
    "    \n",
    "except Exception as e1:\n",
    "    print(f\"❌ 방법 1 실패: {e1}\")\n",
    "    \n",
    "    # 방법 2: CUDA 백엔드 시도\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        lgb_gpu2 = lgb.LGBMRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            device='cuda',  # CUDA 직접 사용\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_gpu2.fit(X_train, y_train)\n",
    "        gpu_time = time.time() - start_time\n",
    "        gpu_pred = lgb_gpu2.predict(X_test)\n",
    "        gpu_rmse = np.sqrt(mean_squared_error(y_test, gpu_pred))\n",
    "        gpu_r2 = r2_score(y_test, gpu_pred)\n",
    "        \n",
    "        print(f\"✅ LightGBM CUDA - 시간: {gpu_time:.2f}초, RMSE: {gpu_rmse:.4f}, R²: {gpu_r2:.4f}\")\n",
    "        print(f\"🚀 속도 향상: {cpu_time/gpu_time:.2f}배\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ 방법 2도 실패: {e2}\")\n",
    "        print(\"💡 LightGBM은 CPU 버전으로 사용하고 XGBoost와 CatBoost GPU를 활용하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7dc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
